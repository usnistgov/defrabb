#!/usr/bin/env python
import argparse
import datetime
import fnmatch
import json
import logging
import os
import re
import subprocess
import sys
from typing import List, Optional, Tuple

import boto3

# Constants
DEFAULT_OUTDIR = "/defrabb_runs/runs_in_progress/"
ARCHIVE_DIR = "/mnt/bbdhg-nas/analysis/defrabb-runs/"
DEFAULT_LOG_LEVEL = "INFO"
VALID_STEPS = ["all", "pipe", "report", "archive", "release"]
LOG_LEVELS = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]


# Custom exceptions
class InvalidRunIDError(ValueError):
    """Exception raised for errors in the format of the RUN ID."""


class InvalidDirectoryError(FileNotFoundError):
    """Exception raised when a specified directory does not exist."""


# Set up logging
def setup_logging(run_dir: str, log_level: str) -> None:
    """Set up logging for the script."""
    log_level = log_level.upper()
    if log_level not in LOG_LEVELS:
        log_level = "INFO"  # Set to default if invalid level provided
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s [%(levelname)s] - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        filename=os.path.join(run_dir, "run.log"),
        filemode="a",
    )
    console = logging.StreamHandler()
    console.setLevel(log_level)
    formatter = logging.Formatter(
        "%(asctime)s [%(levelname)s] - %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
    )
    console.setFormatter(formatter)
    logging.getLogger("").addHandler(console)


def load_release_config(release_config_path: str) -> dict:
    with open(release_config_path, "r") as f:
        return json.load(f)


def parse_arguments() -> Tuple[argparse.Namespace, List[str]]:
    """
    Parse command-line arguments.

    Returns:
        A tuple containing the parsed arguments and any extra arguments.
    """
    parser = argparse.ArgumentParser(
        description="DeFrABB wrapper script for executing and archiving framework",
        formatter_class=argparse.RawTextHelpFormatter,
        epilog="Any additional arguments provided will be passed directly to Snakemake.",
    )
    parser.add_argument(
        "-r",
        "--runid",
        required=True,
        type=str,
        help="Analysis RUN ID, please use following naming convention YYYYMMDD_milestone_brief-id",
    )
    parser.add_argument("-a", "--analyses", type=str, help="defrabb run analyses table")
    parser.add_argument(
        "-o",
        "--outdir",
        type=str,
        default="/defrabb_runs/runs_in_progress/",
        help="Output directory",
    )
    parser.add_argument(
        "-j", "--jobs", type=int, help="Number of jobs used by snakemake"
    )
    parser.add_argument(
        "--archive_dir",
        type=str,
        default=ARCHIVE_DIR,
        help="Directory to copy pipeline run output to for release. Primarily intended for internal NIST use.",
    )
    parser.add_argument(
        "--s3_bucket",
        type=str,
        default=None,
        help="S3 bucket for uploading files (default from release config file)",
    )
    parser.add_argument(
        "--s3_path",
        type=str,
        default=None,
        help="S3 path for uploading files (default from release config file)",
    )
    parser.add_argument(
        "--release_type",
        type=str,
        choices=["local", "s3", "both"],
        default="both",
        help="Type of release: local, s3, or both (default: both)",
    )
    parser.add_argument(
        "--release_config",
        type=str,
        default="config/release.json",
        help="Path to the configuration file (default: config/release.json)",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        choices=LOG_LEVELS,
        default=DEFAULT_LOG_LEVEL,
        help=f"Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)",
    )
    # Create an argument group for better formatting
    group = parser.add_argument_group("workflow steps")

    # Add the 'steps' argument with a detailed description using line breaks
    group.add_argument(
        "-s",
        "--steps",
        type=str,
        choices=["all", "pipe", "report", "archive", "release"],
        default="all",
        metavar="",
        help="""Defining which workflow steps are run:
    all: pipe, report, and archive (default)
    pipe: just the snakemake pipeline
    report: generating the snakemake run report
    archive: generating snakemake archive tarball
    release: copy run output to NAS for upload to Google Drive (internal NIST use-case)""",
    )
    args, unknown_args = parser.parse_known_args()
    return args, unknown_args


def validate_and_set_defaults(args: argparse.Namespace) -> argparse.Namespace:
    """Validate arguments and set default values.

    Args:
        args (argparse.Namespace): Parsed command-line arguments.

    Returns:
        argparse.Namespace: Validated and updated arguments.

    Raises:
        InvalidRunIDError: If the RUN ID format is invalid.
        InvalidDirectoryError: If specified directories do not exist.
    """
    pattern = r"^\d{8}_v\d{1}\.\d{3}_"
    if not re.match(pattern, args.runid):
        raise InvalidRunIDError("Error: Invalid RUN ID format.")
    # Validate analyses file
    if not args.analyses:
        args.analyses = f"config/analyses_{args.runid}.tsv"
    if not os.path.exists(args.analyses):
        raise InvalidDirectoryError(
            f"Error: Analyses file '{args.analyses}' does not exist."
        )
    # Validate output directory
    if not os.path.exists(args.outdir):
        raise InvalidDirectoryError(
            f"Error: Output directory '{args.outdir}' does not exist."
        )
    # Num jobs
    if not args.jobs:
        args.jobs = find_core_limit()
    if args.jobs and args.jobs <= 0:
        raise ValueError("Error: Number of jobs must be a positive integer.")
    # Steps to run
    if args.steps not in VALID_STEPS:
        raise ValueError("Error: Invalid choice for --steps.")
    return args


def validate_and_expand_release_rules(release_config: dict, release_type: str) -> dict:
    release_rules = release_config.get("release_rules", {})
    for key, value in release_rules.items():
        if value == "same as local":
            release_rules[key] = release_rules["local"]
        elif value == "same as s3":
            release_rules[key] = release_rules["s3"]

    for key in ["local", "s3"]:
        if key in release_rules and (
            "include" not in release_rules[key] or "exclude" not in release_rules[key]
        ):
            raise ValueError(f"Include and exclude patterns must be defined for {key}")
    return release_rules


def find_core_limit() -> int:
    """Number of system cpus"""
    return os.cpu_count()


def run_subprocess_command(cmd: List[str], error_message: str) -> None:
    """Run a subprocess command and handle errors."""
    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"{error_message}: {e}")


def execute_snakemake_pipeline(
    args: argparse.Namespace, run_dir: str, extra_args: List[str]
) -> None:
    """Execute Snakemake pipeline."""
    cmd = [
        "snakemake",
        "--use-conda",
        "--printshellcmds",
        "--config",
        f"analyses={args.analyses}",
        "--directory",
        run_dir,
        "--cores",
        str(args.jobs),
    ] + extra_args
    logging.info(f"Pipeline execution command: {' '.join(cmd)}")
    run_subprocess_command(cmd, "Error executing Snakemake pipeline")


def generate_snakemake_report(
    args: argparse.Namespace, run_dir: str, report_name: str, extra_args: List[str]
) -> None:
    """Generate Snakemake report."""
    cmd = [
        "snakemake",
        "--config",
        f"analyses={args.analyses}",
        "--directory",
        run_dir,
        "--report",
        report_name,
    ] + extra_args
    logging.info(f"Command to generate report: {' '.join(cmd)}")
    run_subprocess_command(cmd, "Error generating Snakemake report")


def generate_snakemake_archive(
    args: argparse.Namespace, run_dir: str, smk_archive_path: str, extra_args: List[str]
) -> None:
    """Generate Snakemake archive."""
    if os.path.exists(smk_archive_path):
        logging.warning(
            f"Archive present remove and rerun script if you want to generate a new archive. Archive Path: {smk_archive_path}"
        )
        sys.exit()
    else:
        cmd = [
            "snakemake",
            "--use-conda",
            "--config",
            f"analyses={args.analyses}",
            "--directory",
            run_dir,
            "--cores",
            str(args.jobs),
            "--archive",
            smk_archive_path,
        ] + extra_args
        logging.info(f"Command to generate archive: {' '.join(cmd)}")
        run_subprocess_command(cmd, "Error generating Snakemake archive")


def upload_to_s3(
    run_id: str, run_dir: str, bucket_name: str, s3_path: str, s3_rules: dict
) -> None:
    s3 = boto3.client("s3")
    include_patterns = s3_rules["include"]
    exclude_patterns = s3_rules["exclude"]

    def should_include(file_path: str) -> bool:
        for pattern in include_patterns:
            if fnmatch.fnmatch(file_path, pattern.format(RUNID=run_id)):
                return True
        return False

    def should_exclude(file_path: str) -> bool:
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(file_path, pattern):
                return True
        return False

    for root, _, files in os.walk(run_dir):
        for file in files:
            file_path = os.path.join(root, file)
            relative_path = os.path.relpath(file_path, run_dir)
            if should_include(relative_path) and not should_exclude(relative_path):
                s3_key = os.path.join(s3_path, run_id, relative_path)
                s3.upload_file(file_path, bucket_name, s3_key)
                logging.info(f"Uploaded {file_path} to s3://{bucket_name}/{s3_key}")


def release_run(
    args: argparse.Namespace,
    run_dir: str,
    archive_dir: str,
    bucket_name: str,
    s3_path: str,
    release_type: str,
    release_rules: dict,
) -> None:
    if release_type in ["s3", "both"]:
        try:
            # Upload to S3
            upload_to_s3(args.runid, run_dir, bucket_name, s3_path, release_rules["s3"])

            # Create data manifest
            manifest_path = os.path.join(run_dir, "data_manifest.tsv")
            create_data_manifest(args.runid, bucket_name, s3_path, manifest_path)
        except RuntimeError as e:
            logging.error(f"S3 upload failed: {e}")
            if release_type == "s3":
                sys.exit(1)

    if release_type in ["local", "both"]:
        try:
            # Copy files locally
            local_rules = release_rules["local"]
            cmd = ["rsync", "-rv"]
            for pattern in local_rules["include"]:
                cmd.extend(["--include", pattern.format(RUNID=args.runid)])
            for pattern in local_rules["exclude"]:
                cmd.extend(["--exclude", pattern])
            cmd.extend([f"{run_dir}/", f"{archive_dir}/"])
            subprocess.run(cmd, check=True)
            subprocess.run(
                ["cp", args.analyses, f"{archive_dir}/{args.runid}/"], check=True
            )
            subprocess.run(
                ["cp", "config/resources.yml", f"{archive_dir}/{args.runid}/"],
                check=True,
            )
        except subprocess.CalledProcessError as e:
            logging.error(f"Local copy failed: {e}")
            if release_type == "local":
                sys.exit(1)


def log_git_status() -> None:
    """Log Git status and information."""
    try:
        commit_hash = subprocess.getoutput("git rev-parse HEAD")
        branch_name = subprocess.getoutput("git symbolic-ref --short HEAD")
        logging.info(f"Git latest commit: {commit_hash}")
        logging.info(f"Git branch: {branch_name}")
        uncommitted_changes = subprocess.getoutput("git diff --stat")
        if uncommitted_changes:
            logging.warning("Uncommitted changes detected:")
            logging.warning(uncommitted_changes)
        else:
            logging.info("No uncommitted changes.")
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Error getting Git repo status information: {e}")


def log_conda_environment(run_dir: str) -> None:
    """Generate and log YAML for the specified Conda environment."""
    try:
        # Generate YAML file for the current conda environment
        subprocess.run(
            [
                "conda",
                "env",
                "export",
                "--file",
                os.path.join(run_dir, "environment.yml"),
            ],
            check=True,
        )
        logging.info(f"Conda environment details saved to environment.yml")
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Error getting Conda environment details: {e}")


def create_data_manifest(
    run_id: str, bucket_name: str, s3_path: str, manifest_path: str
) -> None:
    s3 = boto3.client("s3")
    prefix = f"{s3_path}/{run_id}/"

    def determine_analysis(key):
        subdirs = key.split("/")
        if "results" in key:
            return subdirs[3]
        if len(subdirs) > 3:
            return subdirs[2]
        return "NA"

    def determine_file_type(key):
        if key.endswith("tar"):
            return "snakemake_archive"
        if "README" in key and key.endswith(".md"):
            return "README"
        if "config/analyses_" in key and key.endswith(".tsv"):
            return "analysis table"
        if "config/resources.yml" in key:
            return "config"
        if "run.log" in key:
            return "run.log"
        if "environment.yml" in key:
            return "mamba env"
        if "resources/comparison" in key:
            return "comparison vcfs"
        if "resources/exclusions" in key:
            return "exclusion bed"
        if "results/asm_varcalls" in key and key.endswith("dip.bed"):
            return "dip.bed"
        if "results/asm_varcalls" in key and key.endswith("hap1.bam"):
            return "hap1 bam"
        if "results/asm_varcalls" in key and key.endswith("hap1.bam.bai"):
            return "hap1 bam idx"
        if "results/asm_varcalls" in key and key.endswith("hap2.bam"):
            return "hap2 bam"
        if "results/asm_varcalls" in key and key.endswith("hap2.bam.bai"):
            return "hap2 bam idx"
        if (
            "draft_benchmarksets" in key
            and "bench-vars" not in key
            and key.endswith("vcf.gz")
        ):
            if "smvar" in key:
                return "smvar benchmark vcf"
            if "stvar" in key:
                return "stvar benchmark vcf"
        if (
            "draft_benchmarksets" in key
            and "bench-vars" not in key
            and key.endswith("vcf.gz.tbi")
        ):
            if "smvar" in key:
                return "smvar benchmark vcf idx"
            if "stvar" in key:
                return "stvar benchmark vcf idx"
        if "draft_benchmarksets" in key and key.endswith("benchmark.bed"):
            if "smvar" in key:
                return "smvar benchmark bed"
            if "stvar" in key:
                return "stvar benchmark bed"
        else:
            return "other files"

    def determine_ref_id(key):
        for ref in [
            "GRCh37",
            "GRCh38",
            "CHM13",
        ]:
            if ref in key:
                return ref
        return "NA"

    try:
        with open(manifest_path, "w") as file:
            file.write("analysis\tfile_type\tref_id\tsize_Gb\ts3_uri\turl\n")
            continuation_token = None
            is_truncated = True
            while is_truncated:
                if continuation_token:
                    response = s3.list_objects_v2(
                        Bucket=bucket_name,
                        Prefix=prefix,
                        ContinuationToken=continuation_token,
                    )
                else:
                    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
                for item in response.get("Contents", []):
                    key = item["Key"]
                    size_gb = item["Size"] / (1024**3)
                    analysis = determine_analysis(key)
                    file_type = determine_file_type(key)
                    ref_id = determine_ref_id(key)
                    s3_uri = f"s3://{bucket_name}/{key}"
                    url = f"https://{bucket_name}.s3.amazonaws.com/{key}"
                    file.write(
                        f"{analysis}\t{file_type}\t{ref_id}\t{size_gb:.2f}\t{s3_uri}\t{url}\n"
                    )
                is_truncated = response.get("IsTruncated", False)
                continuation_token = response.get("NextContinuationToken", None)
    except Exception as e:
        logging.error(f"Error creating data manifest: {e}")
        raise


def main() -> None:
    args, extra_args = parse_arguments()
    release_config = load_release_config(args.release_config)
    release_rules = validate_and_expand_release_rules(release_config, args.release_type)

    if not args.s3_bucket:
        args.s3_bucket = release_config.get("default_s3_bucket")
    if not args.s3_path:
        args.s3_path = release_config.get("default_s3_path")

    try:
        args = validate_and_set_defaults(args)
    except (InvalidRunIDError, InvalidDirectoryError) as e:
        logging.error(str(e))
        sys.exit(1)

    run_dir = os.path.join(args.outdir, args.runid)
    report_name = os.path.join(run_dir, f"snakemake_report_{args.runid}.zip")
    smk_archive_path = os.path.join(run_dir, f"archive.tar.gz")
    archive_dir = args.archive_dir
    bucket_name = "giab-data"  # Define your S3 bucket name here

    # Set up logging to write to the run directory
    os.makedirs(run_dir, exist_ok=True)
    setup_logging(run_dir, args.log_level)

    ## Starting run and documenting run environment
    logging.info("# Starting DeFrABB Run")
    logging.info("Recording code status and environment")
    log_git_status()
    # Check for Snakemake in the environment and get version
    try:
        snakemake_version = subprocess.check_output(
            ["snakemake", "--version"], text=True
        )
        logging.info(
            f"Snakemake version in Conda environment: {snakemake_version.strip()}"
        )
    except subprocess.CalledProcessError:
        logging.warning(
            "Snakemake not found in the Conda environment. Please run in an activated conda environment with snakemake installed"
        )
        sys.exit(1)
    log_conda_environment(run_dir)

    try:
        if args.steps in ["all", "pipe"]:
            logging.info("Executing Snakemake Pipeline")
            execute_snakemake_pipeline(args, run_dir, extra_args)
        if args.steps in ["all", "report"]:
            logging.info("Generating Snakemake Report")
            generate_snakemake_report(args, run_dir, report_name, extra_args)
        if args.steps in ["all", "archive"]:
            logging.info("Generating Snakemake Archive")
            generate_snakemake_archive(args, run_dir, smk_archive_path, extra_args)
        if args.steps in ["all", "release"]:
            logging.info("Releasing Run")
            release_run(
                args,
                run_dir,
                archive_dir,
                args.s3_bucket,
                args.s3_path,
                args.release_type,
                release_rules,
            )

        logging.info("DeFrABB execution complete!")
    except (RuntimeError, FileNotFoundError, subprocess.CalledProcessError) as e:
        logging.error(str(e))
        sys.exit(1)


if __name__ == "__main__":
    main()
